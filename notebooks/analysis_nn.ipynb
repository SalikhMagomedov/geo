{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import keras.utils.vis_utils\n",
    "import keras_tuner as kt\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import tensorflow as tf\n",
    "from IPython.display import display, clear_output\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from keras import Sequential, layers, losses, metrics\n",
    "from keras import callbacks\n",
    "from keras.api.keras import optimizers\n",
    "\n",
    "import file_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATE_FORMAT = '%d.%m.%Y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def convert_string_to_date(string_date: str) -> dt:\n",
    "    return dt.strptime(string_date, DATE_FORMAT)\n",
    "\n",
    "def normalize_columns_in_dataframe(data: pd.DataFrame, columns: list[str] = None) -> None:\n",
    "    data_columns = data[columns]\n",
    "    min_values = data_columns.min()\n",
    "    max_values = data_columns.max()\n",
    "    data[columns] = (data_columns - min_values) / (max_values - min_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "geochem_data = pd.read_excel(file_helper.get_geo_chem_file_path(), 'dubki_h_tau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "events_catalog_data = pd.read_csv(file_helper.get_events_catalog_file_path(), sep=';').iloc[10000:]\n",
    "events_catalog_data['Date'] = pd.to_datetime(events_catalog_data['Date'], format=DATE_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATE_TARGET_OFFSET = relativedelta(months=3)\n",
    "DATE_TARGET_DURATION = relativedelta(months=1)\n",
    "STARTING_EVENT_CLASS = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def split_data(date_target_offset: relativedelta, date_target_duration: relativedelta, starting_event_class) -> (pd.DataFrame, pd.DataFrame):\n",
    "    preprocessed_data = pd.DataFrame(geochem_data).copy()\n",
    "\n",
    "    normalize_columns_in_dataframe(preprocessed_data, ['events', 'ascend', 'maximum', 'descend', 'minimum'])\n",
    "\n",
    "    breakpoint_date = convert_string_to_date('31.12.2021') - date_target_offset - date_target_duration\n",
    "    reserved_data = preprocessed_data[breakpoint_date < preprocessed_data['to date']].drop(['from date'], axis=1)\n",
    "    preprocessed_data = preprocessed_data[preprocessed_data['to date'] <= breakpoint_date]\n",
    "    preprocessed_data['target'] = preprocessed_data['to date'].map(lambda date: 1 if len(events_catalog_data[\n",
    "        ((date + date_target_offset) <= events_catalog_data['Date'])\n",
    "        & (events_catalog_data['Date'] <= (date + date_target_offset + date_target_duration))\n",
    "        & (starting_event_class <= events_catalog_data['Class'])\n",
    "    ]) > 0 else 0)\n",
    "    preprocessed_data = preprocessed_data.drop(['from date', 'to date'], axis=1)\n",
    "\n",
    "    return preprocessed_data, reserved_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preprocessed_data, reserved_data = split_data(DATE_TARGET_OFFSET, DATE_TARGET_DURATION, STARTING_EVENT_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# display(\n",
    "#     preprocessed_data.tail(),\n",
    "#     reserved_data.head()\n",
    "# )\n",
    "# geochem_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "TRAIN_FRAC = .8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_train_test(data: pd.DataFrame):\n",
    "    x = data.drop(['target'], axis=1)\n",
    "    y = data['target']\n",
    "\n",
    "    x_train, y_train = x.sample(frac=TRAIN_FRAC, random_state=RANDOM_STATE), y.sample(frac=TRAIN_FRAC, random_state=RANDOM_STATE)\n",
    "    x_test, y_test = x.drop(x_train.index), y.drop(y_train.index)\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = generate_train_test(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def model_builder(hp: kt.HyperParameters):\n",
    "    # hp_date_target_offset_months = hp.Int('offset', 1, 12)\n",
    "    # hp_date_target_duration_months = hp.Int('duration', 1, 12)\n",
    "\n",
    "    hp_activation = hp.Choice('activation', values=['linear', 'relu', 'tanh', 'sigmoid'])\n",
    "    hp_units = hp.Int('units', min_value=8, max_value=128, step=8)\n",
    "    hp_activation_1 = hp.Choice('activation_1', values=['linear', 'relu', 'tanh', 'sigmoid'])\n",
    "    hp_units_1 = hp.Int('units_1', min_value=8, max_value=128, step=8)\n",
    "    hp_activation_2 = hp.Choice('activation_2', values=['linear', 'relu', 'tanh', 'sigmoid'])\n",
    "    hp_units_2 = hp.Int('units_2', min_value=8, max_value=128, step=8)\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model = Sequential([\n",
    "        layers.InputLayer((5,), name='input'),\n",
    "        layers.Dense(units=hp_units, activation=hp_activation, name='dense_0'),\n",
    "        layers.Dense(units=hp_units_1, activation=hp_activation_1, name='dense_1'),\n",
    "        layers.Dense(units=hp_units_2, activation=hp_activation_2, name='dense_2'),\n",
    "        layers.Dense(1, name='output')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "        loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[metrics.BinaryAccuracy(name='accuracy')]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project C:\\Users\\saaru\\PycharmProjects\\geo\\data/hypermodel/date_o_d/3/1\\geo_analysis\\oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from C:\\Users\\saaru\\PycharmProjects\\geo\\data/hypermodel/date_o_d/3/1\\geo_analysis\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    model_builder,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=200,\n",
    "    factor=3,\n",
    "    directory=os.path.join(file_helper.get_root_path(), f'data/hypermodel/date_o_d/{DATE_TARGET_OFFSET.months}/{DATE_TARGET_DURATION.months}'),\n",
    "    project_name='geo_analysis'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stop_early = callbacks.EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(x_train, y_train, epochs=100, validation_split=.2, callbacks=[stop_early])\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def search_best_model(seed: int=None):\n",
    "    START_MONTH = 1\n",
    "    END_MONTH = 12\n",
    "\n",
    "    last_score = -1.\n",
    "    best_score = -1.\n",
    "    best_offset, best_duration = (-1, -1)\n",
    "\n",
    "    for offset in range(START_MONTH, END_MONTH + 1):\n",
    "        for duration in range(START_MONTH, END_MONTH + 1):\n",
    "            clear_output(wait=True)\n",
    "            print(f'{\"Offset\":10}: {offset}\\n{\"Duration\":10}: {duration}')\n",
    "            print(f'Best offset: {best_offset}\\nBest duration: {best_duration}')\n",
    "            print(f'Last score: {last_score}')\n",
    "            print(f'Best score: {best_score}')\n",
    "\n",
    "            date_target_offset = relativedelta(months=offset)\n",
    "            date_target_duration = relativedelta(months=duration)\n",
    "\n",
    "            stop_early = callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "            preprocessed_data, reserved_data = split_data(date_target_offset, date_target_duration, STARTING_EVENT_CLASS)\n",
    "            (x_train, y_train), _ = generate_train_test(preprocessed_data)\n",
    "\n",
    "            # split_index = int(x_train.shape[0] * .8)\n",
    "            #\n",
    "            # x_val, y_val = x_train[split_index:], y_train[split_index:]\n",
    "            # x_train, y_train = x_train[:split_index], y_train[:split_index]\n",
    "            #\n",
    "            # train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32).cache().prefetch(tf.data.AUTOTUNE)\n",
    "            # val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(32).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "            tuner = kt.Hyperband(\n",
    "                model_builder,\n",
    "                objective='val_accuracy',\n",
    "                max_epochs=200,\n",
    "                factor=3,\n",
    "                seed=seed,\n",
    "                directory=os.path.join(file_helper.get_root_path(), f'data/hypermodel/date_o_d/{offset}/{duration}'),\n",
    "                project_name='geo_analysis'\n",
    "            )\n",
    "\n",
    "            tuner.search(\n",
    "                # train_dataset,\n",
    "                x_train, y_train,\n",
    "                epochs=100,\n",
    "                validation_split=.2,\n",
    "                # validation_data=val_dataset,\n",
    "                callbacks=[stop_early]\n",
    "            )\n",
    "            score = tuner.oracle.get_best_trials(num_trials=1)[0].score\n",
    "\n",
    "            if best_score < score:\n",
    "                best_score = score\n",
    "                best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "                best_preprocessed = preprocessed_data\n",
    "                best_reserved = reserved_data\n",
    "                best_tuner = tuner\n",
    "                best_offset, best_duration = (offset, duration)\n",
    "            last_score = score\n",
    "\n",
    "    model = best_tuner.hypermodel.build(best_hps)\n",
    "\n",
    "    print(f'{\"Best score\":10}:{best_score}\\n{\"Offset\":10}:{best_offset}\\n{\"Duration\":10}:{best_duration}')\n",
    "\n",
    "    return model, best_preprocessed, best_reserved, best_hps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model, preprocessed_data, reserved_data, best_hps = search_best_model(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation          : tanh\n",
      "units               : 120\n",
      "activation_1        : relu\n",
      "units_1             : 72\n",
      "activation_2        : tanh\n",
      "units_2             : 64\n",
      "learning_rate       : 0.001\n",
      "tuner/epochs        : 200\n",
      "tuner/initial_epoch : 67\n",
      "tuner/bracket       : 2\n",
      "tuner/round         : 2\n",
      "tuner/trial_id      : 0228\n"
     ]
    }
   ],
   "source": [
    "print(*[f'{k:20}: {v}' for k, v in best_hps.values.items()], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model = Sequential([\n",
    "#     layers.Dense(64, name='dense_0'),\n",
    "#     layers.Dense(32, activation=activations.tanh, name='dense_1'),\n",
    "#     layers.Dense(16, activation=activations.tanh, name='dense_2'),\n",
    "#     layers.Dense(1, name='output')\n",
    "# ])\n",
    "# model.compile(\n",
    "#     optimizer=optimizers.Adam(name='adam'),\n",
    "#     loss=losses.BinaryCrossentropy(name='binary_crossentropy', from_logits=True),\n",
    "#     metrics=[metrics.BinaryAccuracy(name='accuracy')]\n",
    "# )\n",
    "\n",
    "model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# (x_train, y_train), (x_test, y_test) = generate_train_test(preprocessed_data)\n",
    "\n",
    "# For performance, change later model input\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    # train_dataset,\n",
    "    epochs=200,\n",
    "    # epochs=best_hps.get('tuner/epochs'),\n",
    "    validation_split=.2,\n",
    "    # verbose=0,\n",
    "    callbacks=[stop_early],\n",
    ")\n",
    "len(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "keras.utils.vis_utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "condition = (convert_string_to_date('1.1.2021') <= reserved_data['to date'])\n",
    "\n",
    "X_pred = reserved_data[condition].drop(['to date'], axis=1)\n",
    "\n",
    "# X_pred = processed_data[condition]\n",
    "\n",
    "result_pred = model.predict(X_pred)\n",
    "result_proba = tf.nn.sigmoid(result_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "figure_x = reserved_data[condition]['to date']\n",
    "figure_x_text = figure_x.map(lambda e: f'[{(e + DATE_TARGET_OFFSET).strftime(\"%d.%m.%Y\")}..{(e + DATE_TARGET_OFFSET + DATE_TARGET_DURATION).strftime(\"%d.%m.%Y\")}]')\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=figure_x,\n",
    "    y=1-result_proba[:,0],\n",
    "    mode='lines',\n",
    "    stackgroup='one',\n",
    "    line=dict(width=.25),\n",
    "    groupnorm='percent',\n",
    "    name='No event',\n",
    "    text=figure_x_text\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=figure_x,\n",
    "    y=result_proba[:,0],\n",
    "    mode='lines',\n",
    "    stackgroup='one',\n",
    "    line=dict(width=.25),\n",
    "    name='Event predicted',\n",
    "    text=figure_x_text\n",
    "))\n",
    "fig.update_layout(\n",
    "    title='Probability of event',\n",
    "    # width=500, height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame(reserved_data[condition])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "result['predicted'] = result_pred\n",
    "result[(result['predicted'] > 0)]['to date'].map(\n",
    "    lambda date:\n",
    "    f'K >= {STARTING_EVENT_CLASS}; Dates [{(date + DATE_TARGET_OFFSET).strftime(\"%d.%m.%Y\")}..{(date + DATE_TARGET_OFFSET + DATE_TARGET_DURATION).strftime(\"%d.%m.%Y\")}]'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}